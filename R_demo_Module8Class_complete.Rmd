---
title: "Module 8 - R demo (solutions)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


# Load penguins data
```{r, include=FALSE}
library(tidyverse);  
library(palmerpenguins); # https://allisonhorst.github.io/palmerpenguins/articles/intro.html
# https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/
```



```{r}
# Let's take a first look at the data
glimpse(penguins);
```


For this demo, we'll be trying to predict the length of a penguin's bill using a linear regression model.

```{r, fig.width=5, fig.height=2.5}
# First step let's look at the association between bill depth and bill length
penguins %>% ggplot(aes(x=bill_depth_mm, y=bill_length_mm)) +
  geom_point()

# Let's get rid of the warning
penguins_clean <- penguins %>% filter(!is.na(bill_depth_mm) & !is.na(bill_length_mm))

# Now we can fit a linear regression model to start exploring this association more deeply
penguins_clean %>% ggplot(aes(x=bill_depth_mm, y=bill_length_mm)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE)

model1 <- lm(bill_length_mm ~ bill_depth_mm, data=penguins_clean)
summary(model1)$coefficients
```

Do you think this fitted linear model is effectively representing the association between bill depth and bill length? What other variable could we add to this model? 
```{r}
#glimpse(penguins_clean)

# New variable to add to the model: species
model2 <- lm(bill_length_mm ~ bill_depth_mm + species, data=penguins_clean)
summary(model2)$coefficients

# Here Adelie is the baseline because it doesn't appear in the summary table
# If we have a categorical variable with M levels, we'll need to create M-1 indicator variables to represent it
# xi2 = I(penguin i is Chinstrap)
# xi3 = I(penguin i is Gentoo)
# So Adelies will have x2=x3=0; Chinstrap will have x2=1 and x3=0 and Gentoo will have x2=0 and x3=1
library(broom)
penguins_clean %>% ggplot(aes(x=bill_depth_mm, y=bill_length_mm, color=species)) +
  geom_point() +
  geom_line(data=augment(model2), aes(y=.fitted))


# What about an interaction term?
model3 <- lm(bill_length_mm ~ bill_depth_mm * species, data=penguins_clean)
summary(model3)$coefficients

penguins_clean %>% ggplot(aes(x=bill_depth_mm, y=bill_length_mm, color=species)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE)

# To make a prediction about the bill length of a Chinstrap penguin with bill depth of 17mm
predict(model3, newdata = tibble(bill_depth_mm=17, species="Chinstrap"))

```

# Comparing the prediction accuracy of multiple models

```{r}
# Set up
set.seed(17); 
n <- nrow(penguins_clean)
training_indices <- sample(1:n, size=round(0.8*n))

penguins_clean <- penguins_clean %>% rowid_to_column() # adds a new ID column

# Create training dataset
train <- penguins_clean %>% filter(rowid %in% training_indices)
y_train <- train$bill_length_mm;

# Testing dataset includes all observations NOT in the training data
test <- penguins_clean %>% filter(!rowid %in% training_indices)
y_test <- test$bill_length_mm;

# Fit models to training data
modA_train <- lm(bill_length_mm ~ bill_depth_mm,           data = train)
modB_train <- lm(bill_length_mm ~ bill_depth_mm + species, data = train)
modC_train <- lm(bill_length_mm ~ bill_depth_mm * species, data = train)


# Make predictions for testing data using training model
yhat_modA_test <- predict(modA_train, newdata = test)
yhat_modB_test <- predict(modB_train, newdata = test)
yhat_modC_test <- predict(modC_train, newdata = test)


# Make predictions for training data using training model
yhat_modA_train <- predict(modA_train, newdata = train)
yhat_modB_train <- predict(modB_train, newdata = train)
yhat_modC_train <- predict(modC_train, newdata = train)


# Calculate RMSE for testing data
modA_test_RMSE <- sqrt(sum((y_test - yhat_modA_test)^2) / nrow(test))
modB_test_RMSE <- sqrt(sum((y_test - yhat_modB_test)^2) / nrow(test))
modC_test_RMSE <- sqrt(sum((y_test - yhat_modC_test)^2) / nrow(test))

# Calculate RMSE for training data
modA_train_RMSE <- sqrt(sum((y_train - yhat_modA_train)^2) / nrow(train))
modB_train_RMSE <- sqrt(sum((y_train - yhat_modB_train)^2) / nrow(train))
modC_train_RMSE <- sqrt(sum((y_train - yhat_modC_train)^2) / nrow(train))

mytable <- tibble(Model = c("A","B","C"),
       RMSE_testdata = c(modA_test_RMSE, modB_test_RMSE, modC_test_RMSE),
       RMSE_traindata = c(modA_train_RMSE, modB_train_RMSE, modC_train_RMSE),
       ratio_of_RMSEs = RMSE_testdata / RMSE_traindata)

library(knitr)
knitr::kable(mytable)
```


### What does it mean if the RMSE based on test data is *smaller* than the RMSE based on the training data?

 - We know that if it is *much larger*, then that suggests that the model may be overfitting the training data, which makes it less effective at generalizing the pattern to new observations 
 - However, because of the random split between training/testing datasets, it's possible that just by chance we end up with a model that happens to have smaller RMSE with the testing data than with the training data!
 - If this happens, than certainly we don't have evidence that the model is overfitting, and in fact, this would suggest that the model is generalizing well to new observations it hasn't seen before (i.e. the predictions it makes for new observations are as accurate or more accurate as what we would expect based on what we saw with the training data). 
 - There was a question on Piazza asking if this was called "underfitting" (i.e. the opposite of overfitting) - a good hypothesis, but *NO*.  "Underfitting" means something different (and, just like overfitting, it is not a good thing).
 







```{r}
# Can we use sample_n() to create training and testing data

train2 <- penguins_clean %>% sample_n(size=0.8*n)
test2 <- penguins_clean %>% filter(!(rowid %in% train2$rowid))


model4 <- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm, data=penguins_clean)
summary(model4)$coefficients

data2 <- tibble(bill_depth_mm=15, species="Adelie")
predict(model2, newdata=data2)
```

